[
  {
    "objectID": "Datathon_Data_Wrangling.html",
    "href": "Datathon_Data_Wrangling.html",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.5.0 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(googlesheets4)\nlibrary(googledrive)\n\n\nAttaching package: 'googledrive'\n\nThe following objects are masked from 'package:googlesheets4':\n\n    request_generate, request_make\n\nlibrary(taxize)\nlibrary(openxlsx)\nlibrary(readxl)\nlibrary(RefManageR)\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nBibOptions(max.names = 100, style = \"text\")\nlibrary(ggalluvial)\nlibrary(lubridate)\n\nLoading required package: timechange\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(clipr)\n\nWelcome to clipr. See ?write_clip for advisories on writing to the clipboard in R.\n\nlibrary(knitr)"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#from-rachels-email-of-june-24-2020",
    "href": "Datathon_Data_Wrangling.html#from-rachels-email-of-june-24-2020",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "From Rachel’s Email of June 24, 2020:",
    "text": "From Rachel’s Email of June 24, 2020:\n\nI generated these four spreadsheets for now (note these are different than our previous list numbering):\n* list1 - divdiv datasets with lat/longs located outside of NCBI (n indivs per dataset > 9)\n* list2 - divdiv datasets for which we haven’t yet searched for lat/long outside of NCBI (n indivs per dataset > 9)\n* list3 - divdiv datasets where we looked for lat/long outside of NCBI and couldn’t find it (n indivs per dataset > 9)\n(aka we know we need to contact author for if we want to try and get lat/long)\n* list4 - eukaryotes sans model organisms removed for which we haven’t yet > assessed data relevance (n indivs per dataset > 4)\n\n\nThe order of operations (I think also laid out in Cynthia’s flow chart in divdiv materials) as I see it is something like:\n1. Assess if dataset is relevant for popgen (natural populations, multiple populations, not hybrids?, etc.)\n2. For those passing #1, try to locate lat/long outside of NCBI and see if sample names can be matched to NCBI metadata we have\n3a. For those passing #2, gather the data from sources outside NCBI\n3b. For those not passing #2, contact authors.\n\n\nlist1 needs to enter the pipeline at step #3a.\nlist2 needs to enter the pipeline at step #2.\nlist3 needs to enter the pipeline at step #3b.\nlist4 needs to enter the pipeline at step #1."
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#pull-out-sra-metadata",
    "href": "Datathon_Data_Wrangling.html#pull-out-sra-metadata",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Pull Out SRA Metadata",
    "text": "Pull Out SRA Metadata\nRachel pulled out some of the metadata that describe the sequencing experiment (why do they call them sequencing “experiments”?) in the mega-field expxml_sra. However, we need to pull out a few more, specifically library_source, platform, instrument_model and protocol_citation. Looks like that field doesn’t contain protocol_citation. That’s OK.\n\npull_sra <- function(listx){\n      plat_inst <- str_match(listx$expxml_sra, pattern = \";Platform instrument_model=\\\"(.+)\\\"&gt;(\\\\w+)&lt;\")\n      library_source <- str_match(listx$expxml_sra, pattern = \"LIBRARY_SOURCE&gt;(\\\\w+)&lt;\")[,2]\n      library_name <- str_match(listx$expxml_sra, pattern = \"LIBRARY_NAME&gt;(\\\\w+)&lt;\")[,2]\n      \n      \n      listx <- cbind(listx, platform = plat_inst[,3])\n      listx <- cbind(listx, instrument = plat_inst[,2])\n      listx <- cbind(listx, library_source = library_source)\n      listx <- cbind(listx, library_name = library_name)\n\n}\n\nlist1 <- pull_sra(list1)\nlist2 <- pull_sra(list2)\nlist3 <- pull_sra(list3)\nlist4 <- pull_sra(list4)"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#winnow-the-fields",
    "href": "Datathon_Data_Wrangling.html#winnow-the-fields",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Winnow the Fields",
    "text": "Winnow the Fields\n(sounds very pastoral)\nI went through each of 139 fields and deleted fields that seemed useless in terms of finding metadata. Remaining fields are in keeps. Fields to delete can now be found below in drops (found with setdiff() and dput()).\n\nkeeps <- c(\"X1\",\"link\",\"Comments_about_spatial_data_location\",\"link_to_published_paper\",\"project_acc_bioprj\",\"organism_biosamp\",\"run_acc_sra\",\"taxid_sra\",\"biosample_acc_sra\",\"X_bioprj\",\"project_data_type_bioprj\",\"project_target_scope_bioprj\",\"project_target_material_bioprj\",\"project_target_capture_bioprj\",\"registration_date_bioprj\",\"project_name_bioprj\",\"project_title_bioprj\",\"project_description_bioprj\",\"relevance_environmental_bioprj\",\"relevance_evolution_bioprj\",\"sequencing_status_bioprj\",\"supergroup_bioprj\",\"lab_name_sra\",\"contact_name_sra\",\"center_name_sra\",\"read_type_sra\",\"library_selection_sra\",\"experiment_acc_sra\",\"sample_acc_sra\",\"study_acc_sra\",\"library_strat_sra\",\"total_number_reads_sra\",\"total_number_base_pairs_sra\",\"createdate_sra\",\"title_biosamp\",\"date_biosamp\",\"modificationdate_biosamp\",\"organization_biosamp\",\"taxonomy_biosamp\",\"sourcesample_biosamp\",\"collectors_info_biosamp\",\"contact_last_name_biosamp\",\"contact_first_name_biosamp\",\"contact_email_biosamp\",\"organization_name_messy_biosamp\",\"collection_date_biosamp\",\"samplename_sampledata_biosamp\",\"lat_long_biosamp\",\"identifiers_biosamp\",\"samplename_identifiers_biosamp\",\"infraspecies_biosamp\",\"ecotype_biosamp\",\"package_biosamp\",\"taxid_taxonomy\",\"scientific_name_taxonomy\",\"myrank_taxonomy\",\"division_taxonomy\",\"full_lineage_taxonomy\",\"publication_ID_bioprj\",\"publication_firstauthor_bioprj\",\"publication_title_bioprj\",\"publication_journalname_bioprj\",\"publication_year_bioprj\",\"publication_volume_bioprj\",\"publication_issue_bioprj\",\"DOI_pubmed\",\"uid_pubmed\",\"pubdate_pubmed\",\"epubdate_pubmed\",\"source_pubmed\",\"lastauthor_pubmed\",\"title_pubmed\",\"sorttitle_pubmed\",\"volume_pubmed\",\"issue_pubmed\",\"pages_pubmed\",\"lang_pubmed\",\"nlmuniqueid_pubmed\",\"issn_pubmed\",\"essn_pubmed\",\"pubtype_pubmed\",\"recordstatus_pubmed\",\"pubstatus_pubmed\",\"fulljournalname_pubmed\",\"elocationid_pubmed\",\"availablefromurl_pubmed\",\"platform\",\"instrument\",\"library_source\", \"biosamp_country\", \"biosamp_locality\")\n\ndrops <- c(\"uid_bioprj\", \"taxid_bioprj\", \"project_type_bioprj\", \n\"project_methodtype_bioprj\", \"relevance_agricultural_bioprj\", \n\"relevance_medical_bioprj\", \"relevance_industrial_bioprj\", \"relevance_model_bioprj\", \n\"relevance_other_bioprj\", \"organism_name_bioprj\", \"organism_strain_bioprj\", \n\"organism_label_bioprj\", \"uid_sra\", \"expxml_sra\", \"scientific_name_sra\", \n\"runs_sra\", \"extlinks_sra\", \"iteration_sra\", \"uid_biosamp\", \"accession_biosamp\",\n\"publicationdate_biosamp\", \"sampledata_biosamp\", \"class_taxonomy\", \n\"family_taxonomy\", \"genus_taxonomy\", \"kingdom_taxonomy\", \"order_taxonomy\", \n\"phylum_taxonomy\", \"class_taxid_taxonomy\", \"family_taxid_taxonomy\", \n\"genus_taxid_taxonomy\", \"kingdom_taxid_taxonomy\", \"order_taxid_taxonomy\", \n\"phylum_taxid_taxonomy\", \"full_fetched_record_bioprj\", \"publication_pages_bioprj\", \n\"attributes_pubmed\", \"pmcrefcount_pubmed\", \"doctype_pubmed\", \n\"booktitle_pubmed\", \"medium_pubmed\", \"edition_pubmed\", \"publisherlocation_pubmed\", \n\"publishername_pubmed\", \"srcdate_pubmed\", \"reportnumber_pubmed\", \n\"locationlabel_pubmed\", \"docdate_pubmed\", \"bookname_pubmed\", \n\"chapter_pubmed\", \"sortpubdate_pubmed\", \"sortfirstauthor_pubmed\", \n\"vernaculartitle_pubmed, issue_pubmed\")\n\nNow I’m going to try a new way to drop columns in dplyr. Using all_of removes potential ambiguity from drops being also a column name. I am then adding in columns for student notations using add_column(). For the marine dataset, some of this work has already been done, so I need to add these columns with appropriate T/F values to the original lists, before summarizing.\n\nlist1 <- select(list1,-all_of(drops)) %>% add_column(.after = \"link\", Metadata_Curator = \"\",\n                                                     Relevant = \"TRUE\", \n                                                     Relevance_Comments = \"\",\n                                                     Paper_Available = \"TRUE\",\n                                                     Paper_Comments = \"\",\n                                                     Authors_Contacted = \"FALSE\",\n                                                     Who_Will_Contact = \"\",\n                                                     Email_Date= \"\",\n                                                     Author_Contact_Comments = \"\",\n                                                     Metadata_Gathered = \"FALSE\",\n                                                     Metadata_Gathered_Comments = \"\",\n                                                     Metadata_Entered = \"FALSE\",\n                                                     Metadata_Entered_Comments= \"\",\n                                                     Validated = \"\",\n                                                     Validation_Comments = \"\",\n                                                     Total_Minutes_Spent= \"\",\n                                                     Quality_Controlled = \"FALSE\",\n                                                     Quality_Controller = \"\",\n                                                     Quality_Control_Comments = \"\")\n\nlist2 <- select(list2,-all_of(drops))  %>% add_column(.after = \"link\", Metadata_Curator = \"\",\n                                                     Relevant = \"TRUE\", \n                                                     Relevance_Comments = \"\",\n                                                     Paper_Available = \"NA\",\n                                                     Paper_Comments = \"\",\n                                                     Authors_Contacted = \"FALSE\",\n                                                     Who_Will_Contact = \"\",\n                                                     Email_Date = \"\",\n                                                     Author_Contact_Comments = \"\",\n                                                     Metadata_Gathered = \"FALSE\",\n                                                     Metadata_Gathered_Comments = \"\",\n                                                     Metadata_Entered = \"FALSE\",\n                                                     Metadata_Entered_Comments = \"\",\n                                                     Validated = \"\",\n                                                     Validation_Comments = \"\",\n                                                     Total_Minutes_Spent= \"\",\n                                                     Quality_Controlled = \"FALSE\",\n                                                     Quality_Controller = \"\",\n                                                     Quality_Control_Comments = \"\")\n\nlist3 <- select(list3,-all_of(drops)) %>% add_column(.after = \"link\", Metadata_Curator = \"\",\n                                                     Relevant = \"TRUE\", \n                                                     Relevance_Comments = \"\",\n                                                     Paper_Available = \"FALSE\",\n                                                     Paper_Comments = \"\",\n                                                     Authors_Contacted = \"FALSE\",\n                                                     Who_Will_Contact = \"\",\n                                                     Email_Date = \"\",\n                                                     Author_Contact_Comments = \"\",\n                                                     Metadata_Gathered = \"FALSE\",\n                                                     Metadata_Gathered_Comments = \"\",\n                                                     Metadata_Entered = \"FALSE\",\n                                                     Metadata_Entered_Comments = \"\",\n                                                     Validated = \"\",\n                                                     Validation_Comments = \"\",\n                                                     Total_Minutes_Spent= \"\",\n                                                     Quality_Controlled = \"FALSE\",\n                                                     Quality_Controller = \"\",\n                                                     Quality_Control_Comments = \"\")\nnonmarinelist <- select(list4,-all_of(drops))\nrm(list4)\n\nmarinelist <- bind_rows(list1,list3,list2)\n\n#write_csv(marinelist, \"../GEOME_Datathon_Dataset_Lists/marinelist_winnowed.csv\")"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#write-bioproject-summaries",
    "href": "Datathon_Data_Wrangling.html#write-bioproject-summaries",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Write BioProject Summaries",
    "text": "Write BioProject Summaries\nI’m going to write these summaries to google sheets, and then manually move them into the working directory for the datathon. Commenting them and turning off eval to ensure they don’t get overwritten.\n\nMarine_bioproject_sheet <- gs4_create(name = \"Marine_SRA_BioProjects2\")\n\nsheet_write(Marine_BioProjects, ss = Marine_bioproject_sheet)\n\nnonMarine_bioproject_sheet <- gs4_create(name = \"NonMarine_SRA_BioProjects\")\n\nsheet_write(nonMarine_BioProjects, ss = nonMarine_bioproject_sheet)\n\nwrite_csv(Marine_BioProjects, \"Marine_BioProjects_20200714.csv\")\n\nwrite_csv(nonMarine_BioProjects, \"nonMarine_BioProjects_20200714.csv\")\n\nAfter writing these sheets:\n\nMove sheets to /GEOME_Datathon/Datathon_Working_Directory/BioProject/Table\nProtect Columns A:E, X:CA. Color columns in between.\nFormat email_date as Date\nSet up validation on Boolean columns\nFreeze up to column F\nSet up Metadata_Gatheredvalidation: (From Author, From Paper)"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#write-biosample-metadata",
    "href": "Datathon_Data_Wrangling.html#write-biosample-metadata",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Write BioSample metadata",
    "text": "Write BioSample metadata\nNow I will loop through all of the BioProjects, using the googledrive package to make a copy of the template googlesheet per BioProject, (itself created from a GEOME template), and fill it with relevant SRA metadata for that BioProject. Found hints about dealing with the Google Sheets API here and here. Basically, I had to get a Google Cloud Platform project, and create a project (called GEOME) and then enable the Google Drive and Google Sheets APIs. Then I created a service account and create a key for the service account, which I downloaded as a JSON. I also had to edit grant access to the Datathon project for the service account (took me awhile to figure this one out). Then below, I just load in the key to authorize the service account to write to the folder for me. Information about trycatch() is here\n\nworkingList <- nonmarinelist\nworkingBioProjects <- nonMarine_BioProjects\npath <- \"GEOME_Datathon/Datathon_Working_Directory/BioProject_Tables/NonMarine_BioSample_Metadata/\"\n\n\n#c(310,318,348)\n\nfor(prj in workingBioProjects$project_acc_bioprj[55]){\n    #prj <- \"PRJNA170681\"\n    #prj <- \"PRJDB7819\" multispecies\n    #prj <- \"PRJNA254780\" working\n    #prj <- \"PRJNA256006\" not working\n    #prj <- \"PRJNA369456\" tricky taxonomy example\n    \n    #grab the index\n    index <- workingBioProjects$project_index[which(workingBioProjects$project_acc_bioprj==prj)]\n    \n    #prj <- workingBioProjects$project_acc_bioprj[55]\n    \n    print(paste(\"Now Starting\",prj,index,sep=\" \"))\n    #copy the template to a new file name after the BioProject\n    print(\"Copying Template\")\n    drive_cp(file = \"GEOME_Datathon/Code/Datathon_BioSample_Template\", path = paste0(path, index,\"_\",prj))\n    \n    # get the `Dribble` (google drive locator) for this new Google sheet\n    gdprj <- drive_get(path = paste0(path, index,\"_\",prj))\n    \n    # grab the various SRA accession IDs and write them to the sheet\n    prjIDs <- workingList[which(workingList$project_acc_bioprj == prj), c(\"project_acc_bioprj\",\n                                                                        \"biosample_acc_sra\",\"run_acc_sra\")]\n    prjIDs <- cbind(project_index=index,prjIDs)\n    \n    # grab more metadata, collapse first and last names, split the genus and species names and \n    #write into the sheet\n    namestax <- workingList[which(workingList$project_acc_bioprj == prj), c(\"contact_first_name_biosamp\",\n   \"contact_last_name_biosamp\",\n   \"samplename_identifiers_biosamp\",\n   \"organism_biosamp\", \n   \"taxonomy_biosamp\", \n   \"infraspecies_biosamp\")]  %>% \n    unite(\"contact_first_name_biosamp\", \"contact_last_name_biosamp\", \n                                    sep = \" \", col = \"collectorList\", remove = T) \n    \n    # Taxize - Create a lookup table\n    print(\"Querying Taxonomy\")\n    \n    # my first try using tax_name() required a separate lookup for every sample!\n    #taxonomy <- tax_name(sci=namestax$organism_biosamp, get = c(\"Phylum\",\"Class\",\"Order\",\"Family\"))\n    #common_name <- sci2comm(sci = namestax$organism_biosamp)\n    \n    #get all unique uids\n    uids <- as.uid(unique(namestax$taxonomy_biosamp), check = F)\n    # this produces a list of data frames, one df per uid\n    taxonomy <- as.list(classification(id = uids, db=\"ncbi\"))\n    # this produces a list of common names. If it can't find a common name though, it assigns `character(0)`\n    # so we overwrite these with NA\n    common_name <- sci2comm(id=uids)\n    common_name[which(common_name==\"character(0)\")]<-NA\n    # classification gives a class called \"classification\", change it back to a list\n    class(taxonomy) <- \"list\"\n    # this will write the common name into each table as a taxonomic rank\n    taxonomy <- Map(function(x,y) {rbind(x,c(y,\"common_name\",0))}, taxonomy, common_name)\n    # put this into a tibble, then unnest it, so that the name of each list is now a column called `uid`\n    # add some dummy rows so that we always have these particular ranks in our taxonomy lookup\n    # drop the id field, and spread it out (pivot_wider replaces spread) so we have the complete \n    # taxonomy for each uid\n    # now we have a lookup table!\n    taxonomy_lookup <- tibble(uid = as.numeric(names(taxonomy)), taxonomy = taxonomy) %>%\n                              unnest(cols = taxonomy) %>% \n                              add_row(uid=0,name = NA, rank = c(\"phylum\",\"class\",\"order\",\"family\", \"genus\", \"species\", \"subspecies\"), \n                                          id = \"0\") %>%\n                              select(-id) %>% \n                              pivot_wider(names_from=rank, values_from = name,\n                                          values_fn = list(name = list)) %>%\n                              unnest(cols = c(phylum, class,  order, family,\n                                              genus, species, subspecies, common_name))\n\n    \n    taxonomized <- left_join(tibble(uid=namestax$taxonomy_biosamp), taxonomy_lookup, \n                             by = \"uid\") %>%\n                select(genus, species,subspecies,phylum,class,order,family,common_name) %>%\n                replace_na(replace = list(subspecies=\"\"))\n\n    #have to write and overwrite here so I can paste subspecies into infraspecies...\n    \n    taxonomized <-  add_column(taxonomized, nomenclaturalCode = paste(\"NCBI queried:\",date(),\n                                                                      sep = \" \"), \n                    scientificNameID = namestax$taxonomy_biosamp, \n                    infraspecies = paste(taxonomized$subspecies,namestax$infraspecies_biosamp,\n                                         sep = \" \")) %>%\n                    mutate_all(~na_if(.,\"character(0)\")) %>% mutate_all(~na_if(.,\"NULL\")) %>% \n                    mutate(infraspecies = str_trim(infraspecies,side = \"both\")) %>%\n                    select(-subspecies)\n                    \n    otherSRA <- workingList[which(workingList$project_acc_bioprj == prj), \n                           c(\"library_selection_sra\",   \"library_strat_sra\",\"read_type_sra\", \n                             \"platform\", \"instrument\", \"study_acc_sra\", \"experiment_acc_sra\",\n                             \"package_biosamp\",\"link\")]\n   \n   # Bind it all together to make a single data frame together with blank columns. \n   # Going to try writing this all at once to reduce the calls to the API\n   metadata<- bind_cols(prjIDs, namestax[,1:2], taxonomized, otherSRA) %>% \n          add_column(.after = 4,sampleEnteredBy = \"\") %>% \n          add_column(.after = 9, locality = \"\", decimalLatitude = \"\", decimalLongitude = \"\",\n                    coordinateUncertaintyInMeters = \"\", country = \"\",\n                    georeferenceProtocol = \"\",\n                    habitat = \"\", microHabitat = \"\", environmental_medium = \"\",\n                    yearCollected = \"\", monthCollected = \"\", dayCollected = \"\",\n                    establishmentMeans = \"\",permitInformation = \"\", associatedReferences = \"\",\n                    preservative = \"\", derivedGeneticDataType = \"\", \n                    derivedGeneticDataURI = \"\", \n                    derivedDataGeneticDataFormat = \"\", \n                    derivedDataGeneticFilename = \"\", derivedGeneticDataRemarks = \"\") %>%\n          add_column(.after = 38, lifeStage = \"\", sex = \"\", otherCatalogNumbers = \"\",\n                      associatedMedia = \"\", samplingProtocol = \"\",  tissueType = \"\",\n                      continentOcean = \"\", island = \"\", maximumDepthInMeters = \"\",\n                      maximumElevationInMeters = \"\", minimumDepthInMeters = \"\", \n                      minimumElevationInMeters = \"\",    stateProvince = \"\", landOwner = \"\"\n                      )\n     \n          metadata <- add_column(metadata,.before = 1,\n                                 sheet_index = str_pad(row.names(metadata),width=4,pad=\"0\"))\n  \n  skip_to_next <- FALSE\n  try_try_again <- FALSE\n\n  print(\"Writing Metadata\")\n\n  tryCatch(range_write(ss = gdprj, metadata, sheet = \"Samples\", range = \"A2:BJ4000\", \n                       col_names = F), \n            error = function(e) { msg <- conditionMessage(e)\n                                  write_csv(tibble(index,\"attempt_1\",msg),\n                                            \"failed.txt\", append = T)\n                                  try_try_again <<- TRUE\n                                  }\n            )\n  \n  #apparently it works if you try it a second time....\n  \n  if(try_try_again) { print(\"Sleeping for 100 seconds to deal with API quota\")\n                      Sys.sleep(time=100)\n                      tryCatch(range_write(ss = gdprj, metadata, \n                                           sheet = \"Samples\", \n                                           range = \"A2:BJ4000\", col_names = F), \n                                error = function(e) { msg <- conditionMessage(e)\n                                  write_csv(tibble(index,\"attempt_2\",msg),\"failed.txt\", \n                                            append = T)\n                                  skip_to_next <<- TRUE\n                                  }\n            ) \n  }\n  \n  if(skip_to_next) { next } \n  \n   print(\"Sleeping for 15 seconds to deal with API quota\")\n    Sys.sleep(time=15)\n}\n\n#write_csv(metadata,\"E0318.csv\")"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#analyzing-failed-googlesheet-write-operations",
    "href": "Datathon_Data_Wrangling.html#analyzing-failed-googlesheet-write-operations",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Analyzing Failed GoogleSheet Write Operations",
    "text": "Analyzing Failed GoogleSheet Write Operations\nWhenever it fails to write, it seems to have something to do with the sample size of the BioProject (# of lines). Also, I can determine which BioProjects have already been found irrelevant.\n\nmarineSRA <- drive_get(path =\n      \"GEOME_Datathon/Datathon_Working_Directory/BioProject_Tables/Marine_SRA_BioProjects\")\n\nnonmarineSRA <- drive_get(path =\n    \"GEOME_Datathon/Datathon_Working_Directory/BioProject_Tables/nonMarine_SRA_BioProjects\")\n\nfailed <- read.csv(\"failed.txt\", header=F, stringsAsFactors = F)\n\nfailed <- failed[which(failed[,2]==\"attempt_2\"),]\n\ncolnames(failed) <- c(\"project_index\",\"write_result\",\"message\")\nfailed$write_result[which(failed$write_result == \"attempt_2\")] <- \"Failure\"\n#113 datasets failed after a second attempt\n\nall5_BioProjects <- bind_rows(Marine_BioProjects[,1:5], nonMarine_BioProjects[,1:5])\n\n\nfailed_analysis <- left_join(all5_BioProjects, failed, by = \"project_index\") %>%\n  replace_na(list(write_result = \"Success\"))\n\n#failed writes seem to be for datasets with larger than average numbers of runs, \n#samples, but not species....\nggplot(failed_analysis) + geom_boxplot(mapping = aes(x = write_result, y = Runs)) +\n  ylim(c(0,1000))\n\nfailed_analysis$write_result <- as.factor(failed_analysis$write_result)\n\ntest <- glm(write_result ~ Runs  , data = failed_analysis, family = \"binomial\")\n\nsummary(test)"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#re-writing-difficult-bioprojects-as-excel-sheets",
    "href": "Datathon_Data_Wrangling.html#re-writing-difficult-bioprojects-as-excel-sheets",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Re-Writing Difficult BioProjects as Excel Sheets",
    "text": "Re-Writing Difficult BioProjects as Excel Sheets\nI can’t figure out why it is failing to write 113 out of 1589 datasets into google sheets. Maybe there is a particular character that it doesn’t like? There is clearly a correlation (see above) between the number of rows (Runs) in the dataset and whether it failed, but that could be because datasets with more rows have a higher probability of including a bad character? [shrug]\nSo I’m just going to adapt my code above to write out Excel worksheets rather than Google Sheets. These can still be edited by the students within Google Sheets.\n\n#split up the failed dataset into marine and nonmarine\nmarineFailed <- failed[grep(\"M\", x = failed$project_index),]\nnonmarineFailed <- failed[grep(\"E\", x = failed$project_index),]\n\n#choose which dataset to work with\nworkingFailed <- nonmarineFailed\nworkingList <- nonmarinelist\nworkingBioProjects <- nonMarine_BioProjects\npath <- \"../Datathon_Working_Directory/BioProject_Tables/NonMarine_BioSample_Metadata/\"\ngdpath <- \"GEOME_Datathon/Datathon_Working_Directory/BioProject_Tables/NonMarine_BioSample_Metadata/\"\n\n\n\n\n#c(310,318,348)\n\nfor(index in workingFailed$project_index[6:103]){\n\n    #index<-\"M0003\"\n    \n    #grab the index\n    prj <- workingBioProjects$project_acc_bioprj[which(workingBioProjects$project_index==index)]\n    \n    print(paste(\"Now Starting\",prj,index,sep=\" \"))\n    #copy the template to a new file name after the BioProject\n    print(\"Copying Excel Template\")\n    #file.copy(from=\"Datathon_BioSample_Template.xlsx\", to = paste0(path, index,\"_\",prj,\".xlsx\") )\n    xlprj <- loadWorkbook(file =\"Datathon_BioSample_Template.xlsx\", \n                          isUnzipped = F)\n    cpprj <- copyWorkbook(xlprj)\n    \n    # get the `Dribble` (google drive locator) for this new Google sheet, and delete the google sheet\n    gdprj <- drive_get(path = paste0(gdpath, index,\"_\",prj))\n    drive_trash(gdprj)\n    \n    # grab the various SRA accession IDs \n    prjIDs <- workingList[which(workingList$project_acc_bioprj == prj), \n                          c(\"project_acc_bioprj\",\"biosample_acc_sra\",\"run_acc_sra\")]\n    prjIDs <- cbind(project_index=index,prjIDs)\n    \n    # grab more metadata, collapse first and last names, split the genus and species names and \n    #write into the sheet\n    namestax <- workingList[which(workingList$project_acc_bioprj == prj),\n                            c(\"contact_first_name_biosamp\", \n                              \"contact_last_name_biosamp\",\n                              \"samplename_identifiers_biosamp\", \n                              \"organism_biosamp\", \n                              \"taxonomy_biosamp\",\"infraspecies_biosamp\")]  %>% \n                unite(\"contact_first_name_biosamp\", \"contact_last_name_biosamp\", \n                                    sep = \" \", col = \"collectorList\", remove = T) \n    \n    \n    # Taxize - Create a lookup table\n    print(\"Querying Taxonomy\")\n    \n    # my first try using tax_name() required a separate lookup for every sample!\n    #taxonomy <- tax_name(sci=namestax$organism_biosamp, get = c(\"Phylum\",\"Class\",\"Order\",\"Family\"))\n    #common_name <- sci2comm(sci = namestax$organism_biosamp)\n    \n    #get all unique uids\n    uids <- as.uid(unique(namestax$taxonomy_biosamp), check = F)\n    # this produces a list of data frames, one df per uid\n    taxonomy <- as.list(classification(id = uids, db=\"ncbi\"))\n    # this produces a list of common names. If it can't find a common name though, it assigns `character(0)`\n    # so we overwrite these with NA\n    common_name <- sci2comm(id=uids)\n    common_name[which(common_name==\"character(0)\")]<-NA\n    # classification gives a class called \"classification\", change it back to a list\n    class(taxonomy) <- \"list\"\n    # this will write the common name into each table as a taxonomic rank\n    taxonomy <- Map(function(x,y) {rbind(x,c(y,\"common_name\",0))}, taxonomy, common_name)\n    # put this into a tibble, then unnest it, so that the name of each list is now a column called `uid`\n    # add some dummy rows so that we always have these particular ranks \n    #in our taxonomy lookup\n    # drop the id field, and spread it out (pivot_wider replaces spread) so we have the complete \n    # taxonomy for each uid\n    # now we have a lookup table!\n    taxonomy_lookup <- tibble(uid = as.numeric(names(taxonomy)), taxonomy = taxonomy) %>%\n                              unnest(cols = taxonomy) %>% \n                              add_row(uid=0,name = NA, \n                                      rank = c(\"phylum\",\"class\",\"order\",\"family\", \"genus\",\n                                                                \"species\", \"subspecies\"), \n                                          id = \"0\") %>%\n                              select(-id) %>% \n                              pivot_wider(names_from=rank, values_from = name, \n                                          values_fn = list(name = list)) %>%\n                              unnest(cols = c(phylum, class,  order, family, genus, \n                                              species, subspecies,\n                                              common_name))\n\n    \n    taxonomized <- left_join(tibble(uid=namestax$taxonomy_biosamp), \n                             taxonomy_lookup, by = \"uid\") %>%\n                    select(genus, species,subspecies,phylum,class,order,\n                                        family,common_name) %>% \n                    replace_na(replace = list(subspecies=\"\"))\n\n    #have to write and overwrite here so I can paste subspecies into infraspecies...\n    \n    taxonomized <-  add_column(taxonomized, \n                               nomenclaturalCode = paste(\"NCBI queried:\",date(), sep = \" \"), \n                    scientificNameID = namestax$taxonomy_biosamp, \n                    infraspecies = paste(taxonomized$subspecies,namestax$infraspecies_biosamp,\n                                         sep = \" \")) %>%\n                    mutate_all(~na_if(.,\"character(0)\")) %>% mutate_all(~na_if(.,\"NULL\")) %>% \n                    mutate(infraspecies = str_trim(infraspecies,side = \"both\")) %>%\n                    select(-subspecies)\n                    \n    otherSRA <- workingList[which(workingList$project_acc_bioprj == prj), \n                           c(\"library_selection_sra\",   \"library_strat_sra\",\"read_type_sra\", \n                             \"platform\", \"instrument\", \"study_acc_sra\", \"experiment_acc_sra\",\n                             \"package_biosamp\",\"link\")]\n   \n   # Bind it all together to make a single data frame together with blank columns. \n   # Going to try writing this all at once to reduce the calls to the API\n   metadata<- bind_cols(prjIDs, namestax[,1:2], taxonomized, otherSRA) %>% \n          add_column(.after = 4,sampleEnteredBy = \"\") %>% \n          add_column(.after = 9, locality = \"\", \n                     decimalLatitude = \"\", decimalLongitude = \"\",                          \n                    coordinateUncertaintyInMeters = \"\", country = \"\", \n                    georeferenceProtocol = \"\",\n                    habitat = \"\", microHabitat = \"\", environmental_medium = \"\",\n                    yearCollected = \"\", monthCollected = \"\", dayCollected = \"\",\n                    establishmentMeans = \"\",permitInformation = \"\", associatedReferences = \"\",\n                    preservative = \"\", derivedGeneticDataType = \"\", derivedGeneticDataURI = \"\",\n                    derivedDataGeneticDataFormat = \"\", \n                    derivedDataGeneticFilename = \"\", derivedGeneticDataRemarks = \"\") %>%\n                    add_column(.after = 38, lifeStage = \"\", sex = \"\", otherCatalogNumbers = \"\",\n                      associatedMedia = \"\", samplingProtocol = \"\",  tissueType = \"\",\n                      continentOcean = \"\", island = \"\", maximumDepthInMeters = \"\",\n                      maximumElevationInMeters = \"\", minimumDepthInMeters = \"\", \n                      minimumElevationInMeters = \"\",    stateProvince = \"\", landOwner = \"\"\n                      )\n     \n          metadata <- add_column(metadata,.before = 1,\n                                 sheet_index = str_pad(row.names(metadata),width=4,pad=\"0\"))\n  \n  skip_to_next <- FALSE\n  try_try_again <- FALSE\n\n  print(\"Writing Metadata\")\n  \n  writeData(wb = cpprj, sheet = \"Samples\", \n            x = metadata, startCol = 1, startRow = 2, colNames = F )\n  \n  tryCatch(\n  saveWorkbook(wb = cpprj, file = paste0(path, index,\"_\",prj,\".xlsx\"), overwrite = TRUE), \n            error = function(e) { msg <- conditionMessage(e)\n                                  write_csv(tibble(index,\"attempt_1\",msg),\n                                            \"failed_excel.txt\", append = T)\n                                  try_try_again <<- TRUE\n                                  }\n            )\n  \n  #apparently it works if you try it a second time....\n  \n  if(try_try_again) {\n                      tryCatch(\n                        saveWorkbook(wb = cpprj, file = paste0(path, \n                                                               index,\"_\",prj,\".xlsx\"),\n                                     overwrite = TRUE), \n                        \n                                error = function(e) { msg <- conditionMessage(e)\n                                  write_csv(tibble(index,\"attempt_2\",msg),\n                                            \"failed_excel.txt\", append = T)\n                                  skip_to_next <<- TRUE\n                                  }\n            ) \n  }\n  \n  if(skip_to_next) { next } \n  \n  \n   \n}"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#species-identified-by-students",
    "href": "Datathon_Data_Wrangling.html#species-identified-by-students",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Species identified by students",
    "text": "Species identified by students\nStudents determined relevance for >1500 projects and put the non-relevant ones into categories, so that provides a starting place.\n\ndroplists <- read.csv(\"NonWildSpecies/raw_student_generated_drop_lists.csv\")\n\n\ndroplists$Student_HumPath %>% str_sort() %>% unique()\n\ndom <- sort(unique(droplists$Student_Domesticated[droplists$Student_Domesticated != \"\" ]))\ndom_common <- sci2comm(dom, db = \"itis\")\ndom_common <-  paste(lapply(dom_common,function(x) if(identical(x, character(0))) NA_character_ else x),sep=\",\")\ndomesticated <- cbind(dom,dom_common)\n\nhumpath <- sort(unique(droplists$Student_HumPath[droplists$Student_HumPath != \"\" ]))\nhumpath_common <- sci2comm(humpath, db = \"itis\")\nhumpath_common <-  paste(lapply(humpath_common,function(x) if(identical(x, character(0))) NA_character_ else x),sep=\",\")\nhumanPathogen <- cbind(humpath,humpath_common)\n\nlab <- sort(unique(droplists$Student_Lab[droplists$Student_Lab != \"\" ]))\nlab_common <- sci2comm(lab, db = \"itis\")\nlab_common <-  paste(lapply(lab_common,function(x) if(identical(x, character(0))) NA_character_ else x),sep=\",\")\nlabSpecies <- cbind(lab,lab_common)\n\nwrite.csv(domesticated,\"NonWildSpecies/domesticated_students_taxized.csv\", row.names = F, quote = F)\nwrite.csv(humanPathogen,\"NonWildSpecies/humanPathogen_students_taxized.csv\", row.names = F, quote = F)\nwrite.csv(labSpecies,\"NonWildSpecies/labSpecies_students_taxized.csv\", row.names = F, quote = F)\n\nNow I’m going to scrape some lists from the web. I pulled in data from:\n\nFAOSTAT for crop plants, FAO Domesticated Animal Diversity Information System\nDAD-IS for animals.\nFAO List of Cereal - Downloaded into domesticatedPlants_Wikipedia_Cereals_Other.csv\nWikipedia List of Infectious Diseases for human pathogens\nWikipedia List of Domesticated Animals\nWikipedia List of Fruits\nWikipedia List of Domesticated Plants Manually copied extras into domesticatedPlants_Wikipedia_Cereals_Other.csv\nWikipedia List of Model Organisms"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#human-pathogens",
    "href": "Datathon_Data_Wrangling.html#human-pathogens",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Human Pathogens",
    "text": "Human Pathogens\n\n#nifty code from Hadley to read in htmls tables\ndisease_html <- read_html(\"https://en.wikipedia.org/wiki/List_of_infectious_diseases\")\ndiseases <- disease_html %>% html_node(\"table\") %>% html_table()\ndiseases$`Infectious agent` <- str_remove(diseases$`Infectious agent`,\"usually \") %>% \n                                              str_remove(\"most \") %>% \n                                              str_remove(\"commonly \")  %>% \n                                              str_remove(\"Group A \")  %>% \n                                              str_remove(\"multiple \") %>%\n                                              str_remove(regex(\"\\\\(.+\\\\)\"))\n\n#need to find the kingdom so we can weed out the bacteria\nkingdoms <- tax_name(diseases$`Infectious agent`, get = \"kingdom\")\ndiseases <- cbind(diseases,kingdoms)\ndiseases <- diseases[-grep(x = diseases$kingdom,pattern = \"Bacteria\"),]\n# and viruses\ndiseases <- diseases[-grep(x = diseases$`Infectious agent`,pattern = \"vir[iu]\",ignore.case = T, perl = T),]\n\n#write.csv(diseases, \"humanPathogen_wikipedia_diseases_kingdoms.csv\", row.names = F, quote = F)\nwrite.table(diseases, \"NonWildSpecies/humanPathogen_wikipedia_diseases_eukaryotes.tsv\", row.names = F, quote = F, sep = \"\\t\")"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#domesticated-species",
    "href": "Datathon_Data_Wrangling.html#domesticated-species",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Domesticated Species",
    "text": "Domesticated Species\n\nAnimals\n\n# These FAO lists didn't work because its damn near impossible to go from common to Linnaean name programatically\n#animals <- read.csv(\"FAO_DomesticAnimalDiversitySystem.csv\", row.names = F)\n#animals_common <- sort(unique(animals$ISO3))\n#animals_linnaean2 <- comm2sci(animals_common, db = \"itis\")\n\n#plants <- read.csv(\"FAOSTAT_data_1-21-2021_crops.csv\")\n#plants_common <- sort(unique(plants$Item))\n#plants_linnaean <- comm2sci(plants_common, db = \"itis\")\n\n\nanimal_html <- read_html(\"https://en.wikipedia.org/wiki/List_of_domesticated_animals\")\nanimals <- animal_html %>% html_node(\"table\") %>% html_table()\nanimals_linnaean <- str_extract(animals$`Species and subspecies`, regex(\"\\\\((.+)\\\\)\")) %>% str_extract(regex(\"[:alpha:]+ [:alpha:]+\"))\nanimals<-cbind(animals_linnaean, animals)\n\nwrite.csv(animals, \"NonWildSpecies/domesticatedAnimals_wikipedia.csv\", row.names = F)\n\nread.csv(\"NonWildSpecies/FAO_aquaculture.csv\")\n\n\n\nPlants\n\nfruits_html <- read_html(\"https://en.wikipedia.org/wiki/List_of_culinary_fruits\")\n#multiple tables in this page, but purrr comes to the rescue!\nfruits_tables <- fruits_html %>% html_nodes(\"table\") %>% map(html_table) %>% bind_rows()\nfruits_tables$Category<-\"Domesticated Fruit\"\nfruits_tables<-fruits_tables[,-3]\nfruits_tables <- fruits_tables[,c(2,1,3)]\nnames(fruits_tables) <- c(\"Species\",\"Common\",\"Category\")\nwrite.csv(fruits_tables,\"domesticatedPlants_Wikipedia_Fruits.csv\", row.names = F)\n\nvegetables_html <- read_html(\"https://en.wikipedia.org/wiki/List_of_vegetables\")\nvegetables_tables <- vegetables_html %>% html_nodes(\"table\") %>% \n                      map(html_table) %>% bind_rows()\n#remove the column names that read in wrong\nvegetables_tables <- vegetables_tables[-grep(\"Species\",vegetables_tables$X2),]\nnames(vegetables_tables) <- c(\"Common name\", \"Species name\")\nvegetables_tables$Category<-\"Domesticated Vegetable\"\nvegetables_tables$`Species name` <- str_remove(vegetables_tables$`Species name`, \n                                               \"\\\\(.+\\\\)\") %>% \n                                                          str_remove(\"\\\\[>+\\\\]\")\nvegetables_tables <- vegetables_tables[,c(2,1,3)]\nnames(vegetables_tables) <- c(\"Species\",\"Common\",\"Category\")\nwrite.csv(vegetables_tables,\"NonWildSpecies/domesticatedPlants_Wikipedia_Vegetables.csv\",\n          row.names = F)\n\n\nother_tables <- read.csv(\"NonWildSpecies/domesticatedPlants_Wikipedia_Cereals_Other.csv\")\n\n\n\ndomesticatedPlants <- rbind(fruits_tables, vegetables_tables, other_tables)\n\n\n\nwrite.csv(domesticatedPlants,\"NonWildSpecies/domesticatedPlants_Wikipedia_all.csv\",row.names=F)"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#model-organisms",
    "href": "Datathon_Data_Wrangling.html#model-organisms",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Model Organisms",
    "text": "Model Organisms\n\nmodel_html <- read_html(\"https://en.wikipedia.org/wiki/List_of_model_organisms\")\n#used selector gadget (https://selectorgadget.com) to pick out the scientific names, which were italicized. \nmodel_list <- model_html %>% html_nodes('.tright+ ul i a , i a+ a , ul a+ i , h3+ ul i a , #Protists , #Eukaryotes') %>% html_text()\n# Then edited manually to remove a few other stray italics\nmodel_list_edit <- edit(model_list)\n\nmodel_list_clean <- sort(unique(model_list_edit))\nmodel_common <- sci2comm(model_list_clean, db = \"itis\")\nmodel_common <-  paste(lapply(model_common,function(x) if(identical(x, character(0))) NA_character_ else x),sep=\",\")\nmodelSpecies <- cbind(model_list_clean,model_common)\n\nwrite.csv(modelSpecies,\"NonWildSpecies/labSpecies_wikipedia_taxized.csv\", row.names = F)"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#combining-lists",
    "href": "Datathon_Data_Wrangling.html#combining-lists",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Combining Lists",
    "text": "Combining Lists\n\n# Model Species\nlabStudents <- read.csv(\"NonWildSpecies/labSpecies_students_taxized.csv\", row.names = 1)\nlabWiki <- read.csv(\"NonWildSpecies/labSpecies_wikipedia_taxized.csv\")\n\nlabSpecies <- left_join(labStudents,labWiki,by=c(\"lab\" = \"model_list_clean\"))\n\nlabSpecies <- labSpecies[,-3]\nlabSpecies$Category <- \"Model Species\"\nnames(labSpecies) <- c(\"Species\",\"Common\",\"Category\")\n\nwrite.csv(labSpecies,\"labSpecies_joined.csv\")\n\n# Human Pathogens\nhumpathStudents <- read.table(\"NonWildSpecies/humanPathogen_students_taxized.csv\", \n                              sep = \",\", header = T, stringsAsFactors = F, row.names = 1)\nhumpathWiki <- read.table(\"NonWildSpecies/humanPathogen_wikipedia_diseases_eukaryotes.tsv\", \n                          sep = \"\\t\", header = T, stringsAsFactors = F)\nhumpathSpecies <- left_join(humpathWiki,humpathStudents,by = c(\"Infectious.agent\"=\"humpath\"))\nhumpathSpecies <- humpathSpecies[,1:2]\nhumpathSpecies$Category <- \"HumanPathogen Species\"\nnames(humpathSpecies) <- c(\"Species\",\"Common\",\"Category\")\n\nwrite.csv(humpathSpecies,\"NonWildSpecies/humanpathogenSpecies_joined.csv\", row.names=F)\n\n# Domesticated Species\ndomesticatedPlants <- read.csv(\"NonWildSpecies/domesticatedPlants_Wikipedia_all.csv\")\ndomesticatedAnimals <- read.csv(\"NonWildSpecies/domesticatedAnimals_wikipedia.csv\")\ndomesticatedAnimals <- domesticatedAnimals[,1:2]\ndomesticatedAnimals$Category = \"Domesticated Animal\"\nnames(domesticatedAnimals) <- c(\"Species\",\"Common\",\"Category\")\ndomesticatedWiki <- rbind(domesticatedAnimals, domesticatedPlants)\ndomesticatedStudents <- read.csv(\"NonWildSpecies/domesticated_students_taxized.csv\", \n                                 row.names = 1)\n\ndomesticatedSpecies <- full_join(domesticatedWiki, domesticatedStudents, \n                                 by = c(\"Species\"= \"dom\"))\n\nwrite.csv(domesticatedSpecies,\"NonWildSpecies/domesticatedSpecies_joined.csv\", row.names = F)\n\nI took these joined lists and hand-edited them, deleting species which, based on a brief google search seemed to have significant natural populations. One example of this was deleting three-spined stickleback (Gasterosteus aculeatus), which is an evolutionary model organism, but still has significant natural populations that are frequently studied as well. This reasoning was in keeping with our use of the wording “potentially relevant” to genetic diversity of natural populations. For genera which I judged to be entirely or almost entirely non-natural, I generally only included the generic name. I left the × character for hybrids."
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#improving-sources",
    "href": "Datathon_Data_Wrangling.html#improving-sources",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Improving Sources",
    "text": "Improving Sources\nUsing just Wikipedia and species identified by students for each category is not exactly authoritative. I’ve scoured the literature, the web, and FAO reports for more authoritative lists of each of the three categories.\n\nFAO The state of the world’s biodiversity for food and agriculture 2019\nFAO The state of the world’s plant genetic resources for food and agriculture 2011\nThe second report on the state of the world’s animal genetic resources for food and agriculture State of Livestock Diversity Table 1A1\nMasfeld’s World Database of Agricultural and Horticultural Crops (Cited in FAO)\nARS-GRIN (Germplasm Resources Information Network) World Economic Plants\nFAO The state of the world’s fisheries and aquaculture 2018: Tables 7 & 8\nVEuPathDB - Eukaryotic Pathogen, Vector & Host Informatics Resource - Organisms table.\n\n\n# Start by reading in the list of species that I created from Wikipedia and Students\nallwikispecies <- read.csv(\"NonWildSpecies/nonNaturalSpecies_joined_preliminary.csv\")\n\n\n# Got a solid list of eukaryotic pathogens from VEUpathDB\n\nveupathdb <- read.csv(\"NonWildSpecies/VEUpathDB_organisms.csv\") %>% map(str_trim)\nveupathdb_common <- sci2comm(veupathdb$Species)\nveupathdb_common <-  paste(lapply(veupathdb_common,function(x) if(identical(x, character(0))) NA_character_ else x),sep=\",\")\n\nveupathdb_final <- data.frame(Species = veupathdb$Species, Common = veupathdb_common )\nveupathdb_final$Category <- \"Human Pathogen VEuPathDB\"\nwikihumpath <- allwikispecies[grep(\"HumanPathogen\",wikihumpath$Category),]\nwikihumpath$Category <- (\"Human Pathogen Wikipedia\")\nallhumpath <- full_join(wikihumpath, veupathdb_final)\nallhumpath <- unique(allhumpath)\n\nwrite.table(allhumpath,file = \"NonWildSpecies/HumanPathogens_Final.tsv\",sep = \"\\t\", quote=F)\n\n\n# Got top aquaculture species from FAO\n\nFAOaquaculture <- read.csv(\"NonWildSpecies/FAO_aquaculture.csv\",comment.char = \"#\") %>% map(str_trim) %>% data.frame()\n\n# Got a partial list of domesticated animals from FAO\n\nFAOanimals <- read.csv(\"NonWildSpecies/FAO_Domesticated_Animals.csv\",comment.char = \"#\") %>% map(str_trim) %>% data.frame()\n\n#Found World Economic Plants from the ARS-GRIN database and downloaded Human Food plants and lawn and turf plants\n\nGRIN_food <- read.table(\"./NonWildSpecies/ARS_GRIN_HumanFood.txt\", header=T, sep = \"\\t\", comment.char = \"#\")\nGRIN_food_sp <- str_extract(GRIN_food$Species, pattern=regex(\"^[:alpha:]+ ×* *[:alpha:]+\"))\nGRIN_food_sp <- sort(unique(GRIN_human_sp))\nGRIN_food_common <- sci2comm(GRIN_food_sp, db = \"itis\")\nGRIN_food_common2 <-  paste(lapply(GRIN_food_common,function(x) if(identical(x, character(0))) NA_character_ else x),sep=\",\")\nGRIN_foodplants <- data.frame(Species = GRIN_food_sp, Common = GRIN_food_common2 )\nGRIN_foodplants$Category <- \"Domesticated Plant GRIN Food\"\n\nGRIN_lawn <- read.table(\"./NonWildSpecies/ARS_GRIN_lawn.txt\", header=T, sep = \"\\t\", comment.char = \"#\")\nGRIN_lawn_sp <- str_extract(GRIN_lawn$Species, pattern=regex(\"^[:alpha:]+ ×* *[:alpha:]+\"))\nGRIN_lawn_sp <- sort(unique(GRIN_lawn_sp))\nGRIN_lawn_common <- sci2comm(GRIN_lawn_sp, db = \"itis\")\nGRIN_lawn_common2 <-  paste(lapply(GRIN_lawn_common,function(x) if(identical(x, character(0))) NA_character_ else x),sep=\",\")\nGRIN_lawnplants <- data.frame(Species = GRIN_lawn_sp, Common = GRIN_lawn_common2 )\nGRIN_lawnplants$Category <- \"Domesticated Plant GRIN Lawn\"\n\nGRIN_plants<-rbind(GRIN_foodplants, GRIN_lawnplants)\n\n\nwikidomesticated <- allwikispecies[grep(\"Domesticated\", allwikispecies$Category),]\nwikidomesticated$Category[-grep(\"Student\",wikidomesticated$Category)] <- paste(wikidomesticated$Category[-grep(\"Student\",wikidomesticated$Category)], \"Wikipedia\", sep = \" \")\n\n# Rachel found a list in Khoury et al. 2016 that takes the FAOstat categories (roughly) and gives latin binomials. I edited it into machine readable format\nkhouryplants <- read.csv(\"./NonWildSpecies/Khouri_etal_2016_EDC_edit.csv\") %>% map(str_trim) %>% data.frame()\n\n#joining these sources together in rough order of authoritativeness, dropping anything that doesn't have a common name in ITIS\n# or from one of the sources (many GRIN things)\nalldomesticated <- full_join(khouryplants, FAOaquaculture) %>%  full_join(FAOanimals) %>% full_join(GRIN_plants)  %>% \n                    full_join(wikidomesticated) %>% distinct(Species, .keep_all=T) %>% arrange(Species) %>% \n                    na_if(\"\") %>% na_if(\"NA\") %>%  drop_na(Common)\n\n# Sort them in this order of authority\nalldomesticated$Category <- factor(alldomesticated$Category, \n                                   levels = c(\"Domesticated Plant Khouri et al. 2016\", \n                                              \"Domesticated Aquaculture FAO\",  \n                                              \"Domesticated Animal FAO\",\n                                              \"Domesticated Plant GRIN Food\", \n                                              \"Domesticated Plant GRIN Lawn\", \n                                              \"Domesticated Animal Wikipedia\",\n                                              \"Domesticated Plant Cereal Wikipedia\", \n                                              \"Domesticated Plant Vegetable Wikipedia\",\n                                              \"Domesticated Plant Fruit Wikipedia\",\n                                              \"Domesticated Plant Other Wikipedia\",\n                                              \"Domesticated Fungus Wikipedia\",\n                                              \"Domesticated Plant Student\",\n                                              \"Domesticated Animal Student\"))      \n\nalldomesticated <- alldomesticated[order(alldomesticated$Category),]\n\nwrite.table(alldomesticated,\"NonWildSpecies/Domesticated_AllSources.tsv\",sep=\"\\t\",quote=F, row.names = F)\n\nI then went through this table and basically dropped things I had never heard of, for lack of a better criterion, with the idea being that the minor items I was dropping might have wild populations. I ensured that there weren’t duplicates within each of the three categories, but allowed duplicates such as S. cerevisiae across the three categories. For genera where I determined that the entire genus was cultivated (e.g. Citrus, Triticum) I kept only the genus name.\nThen I went back and below separated out the source information into its own column so that we can sort by that, or filter on it later if necessary. Because of how I joined the data, each item is attributed to its highest source of authority.\n\nall <- read.csv(file = \"NonWildSpecies/nonWildSpecies_final.tsv\", header = T, \n                sep = \"\\t\", stringsAsFactors = F)\nnames(all) <- c(\"Species\", \"Common\", \"Old\")\nall$Category <- str_extract(all$Old, regex(\"^[:alpha:]+ [:alpha:]+\"))\nall$Source <- str_replace(all$Old, regex(\"^[:alpha:]+ [:alpha:]+ \"), \"\")\nall$Old <- NULL\n\nwrite.table(all,file = \"NonWildSpecies/nonWildSpecies_final_sources.tsv\", \n            sep = \"\\t\", quote=F,row.names = F)"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#summarize",
    "href": "Datathon_Data_Wrangling.html#summarize",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Summarize",
    "text": "Summarize\n\nbiosampsQC_nodups <- read_tsv(\"datathon1_working_files/paper2_all_QCd_biosample_list.tsv\",\n                              col_types = cols(.default =   col_character()))\n\nbiosamps<-read_tsv(\"datathon1_working_files/paper2_all_biosample_list.tsv\",col_types = cols(.default = col_character()))\n                                                                                                              \n\nDatathon_Species <- biosampsQC_nodups %>% group_by(project_acc_bioprj,specificEpithet) %>% \n                      summarize(\n                              genus = first(genus),\n                              phylum = first(phylum),\n                              class = first(class),\n                              specificEpithet = first(specificEpithet),\n                              project_index = first(project_index),\n                              project_acc_bioprj = first(project_acc_bioprj),\n                              BioSamples = n_distinct(biosample_acc_sra),\n                              Localities = n_distinct(locality),\n                              Countries = n_distinct(country),\n                              sampleEnteredBy = first(sampleEnteredBy),\n                              collectorList = first(collectorList),\n                              associatedReferences = first(associatedReferences),\n                              establishmentMeans = paste(unique(establishmentMeans),\n                                                         collapse = \"|\"),\n                              habitat = paste(unique(habitat),\n                                                         collapse = \"|\"),\n                              library_selection_sra = paste(unique(library_selection_sra),\n                                                         collapse = \"|\"),\n                              library_strat_sra = paste(unique(library_strat_sra),\n                                                         collapse = \"|\"),\n                              sequencing_platform = paste(unique(sequencing_platform),\n                                                         collapse = \"|\"),\n                              instrument_model = paste(unique(instrument_model),\n                                                         collapse = \"|\")\n                      )\n\nbiosamps$QC <- as.logical(biosamps$QC)\nbiosamps_old <- biosamps %>% filter(QC) \nbiosampsQC_nodups %>%  summarize(bioprojects = length(unique(project_acc_bioprj)),\n                                      biosamps = length(project_index), \n                             nSpecies= length(unique(specificEpithet)), \n                             phyla = length(unique(phylum)))\n\nnewBioProjects <- setdiff(biosampsQC_nodups$project_index,biosamps_old$project_index)\n\nnewBioProjects <- setdiff(biosampsQC_nodups$project_acc_bioprj,biosamps$project_acc_bioprj)\n\n\n\n#write_tsv(Datathon_Species, \"datathon1_working_files/GEOME_Datathon_Species_datasets.tsv\")                          \n                              \ndf$Metadata_Gathered[df$project_index %in% unique(biosampsQC_nodups$project_index)] \n\ntest <- df %>% filter(df$project_index %in% unique(biosampsQC_nodups$project_index)) %>% summarize()\n\nbiosampsQC_nodup_gathered <- left_join(biosampsQC_nodups,df[,c(\"project_index\",\"Metadata_Gathered\")],by = \"project_index\") \n\n biosampsQC_nodup_gathered %>% filter(project_acc_bioprj %in% newBioProjects) %>% count(Metadata_Gathered)"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#analysis-1",
    "href": "Datathon_Data_Wrangling.html#analysis-1",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Analysis 1",
    "text": "Analysis 1\nP(metadata in SRA or paper aka before author contact) ~ registration date of BioProject\nmust have year & (locality OR coords) for spatiotemp to be counted as present; inclusion criteria - relevant = T plus a handful of other decisions to get to the N = 492 bioprjs\n\nm1Betas <- read_delim(\"figures_and_outputs/analysis1_betaTable.csv\",\n                      delim = \" \") %>% \n          #split the CI column\n          mutate(CI95 = str_extract_all(`95% CI`,\"-*[0-9]\\\\.[0-9]+\")) %>%\n          # move the resulting list column into two columns\n          unnest_wider(CI95, names_sep = \"_\") %>% \n          #change to numeric\n          mutate(across(starts_with(\"CI\"),as.numeric)) %>% \n          #exponentiate to get odds ratio\n          mutate(OddsMean = exp(mean), OddsLo95CI = exp(CI95_1), \n                 OddsHi95CI = exp(CI95_2))\n\nRows: 12 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): model, 95% CI\ndbl (3): mean, median, p-value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nkable(m1Betas[,c(1,2,5,8:10)], digits=3)\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nmean\np-value\nOddsMean\nOddsLo95CI\nOddsHi95CI\n\n\n\n\nPublication DOI\n0.775\n0.000\n2.171\n1.351\n3.789\n\n\nDerived genetic data\n-0.029\n0.556\n0.971\n0.875\n1.090\n\n\nPreservative used\n-0.111\n0.036\n0.895\n0.804\n0.991\n\n\nPermit ID\n-0.096\n0.146\n0.908\n0.804\n1.025\n\n\nCollection year\n-0.133\n0.008\n0.875\n0.792\n0.967\n\n\nEnviro. medium\n0.176\n0.032\n1.192\n1.016\n1.426\n\n\nHabitat\n0.141\n0.044\n1.151\n1.006\n1.330\n\n\nCountry\n-0.012\n0.884\n0.988\n0.820\n1.210\n\n\nLat./long.\n0.107\n0.052\n1.113\n0.999\n1.237\n\n\nPlace name\n-0.026\n0.694\n0.974\n0.856\n1.124\n\n\nSample ID\n0.016\n0.830\n1.016\n0.894\n1.162\n\n\nSpatiotemporal\n-0.145\n0.006\n0.865\n0.775\n0.964"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#analysis-2",
    "href": "Datathon_Data_Wrangling.html#analysis-2",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Analysis 2",
    "text": "Analysis 2\nP(author response|authors contacted = T) ~ registration date of BioProject\n\nm2Betas <- read_delim(\"figures_and_outputs/analysis2_betaTable.csv\",\n                      delim = \" \") %>% \n          #split the CI column\n          mutate(CI95 = str_extract_all(`95% CI`,\"-*[0-9]\\\\.[0-9]+\")) %>%\n          # move the resulting list column into two columns\n          unnest_wider(CI95, names_sep = \"_\") %>% \n          #change to numeric\n          mutate(across(starts_with(\"CI\"),as.numeric)) %>% \n          #exponentiate to get odds ratio\n          mutate(OddsMean = exp(mean), OddsLo95CI = exp(CI95_1), \n                 OddsHi95CI = exp(CI95_2))\n\nRows: 1 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (1): 95% CI\ndbl (4): model, mean, median, p-value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nkable(m2Betas[,c(1,2,5,8:10)], digits=3)\n\n\n\n\nmodel\nmean\np-value\nOddsMean\nOddsLo95CI\nOddsHi95CI\n\n\n\n\n1\n0.227\n0.002\n1.255\n1.12\n1.412"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#analysis-3",
    "href": "Datathon_Data_Wrangling.html#analysis-3",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Analysis 3",
    "text": "Analysis 3\nP(recovered partial data|author response = T) ~ registration date of BioProject Recovered metadata = T if spatiotemp (collection date AND (coords or locality)) increases from MID to POST\nmust have gained year OR (locality OR coords) metadata; any amount of gain counts (e.g. F -> SOME, SOME -> T, F -> MOST, etc.); inclusion criteria - authors responded and dataset was missing any amount of data in locality or year or coords (after looking in papers)\n\nm3Betas <- read_delim(\"figures_and_outputs/analysis3_betaTable.csv\",\n                      delim = \" \") %>% \n          #split the CI column\n          mutate(CI95 = str_extract_all(`95% CI`,\"-*[0-9]\\\\.[0-9]+\")) %>%\n          # move the resulting list column into two columns\n          unnest_wider(CI95, names_sep = \"_\") %>% \n          #change to numeric\n          mutate(across(starts_with(\"CI\"),as.numeric)) %>% \n          #exponentiate to get odds ratio\n          mutate(OddsMean = exp(mean), OddsLo95CI = exp(CI95_1), \n                 OddsHi95CI = exp(CI95_2))\n\nRows: 12 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): model, 95% CI\ndbl (3): mean, median, p-value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nkable(m3Betas[,c(1,2,5,8:10)], digits=3)\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nmean\np-value\nOddsMean\nOddsLo95CI\nOddsHi95CI\n\n\n\n\ndatathonderivedGeneticDataX\n-0.372\n0.076\n0.689\n0.416\n1.050\n\n\ndatathonpermitInformation\n-0.555\n0.058\n0.574\n0.270\n1.003\n\n\ndatathonyearCollected\n-0.174\n0.048\n0.840\n0.696\nNA\n\n\ndatathonmaterialSampleID\n-0.259\n0.204\n0.772\n0.528\n1.138\n\n\ndatathonpreservative\n-0.438\n0.014\n0.645\n0.418\n0.922\n\n\ndatathoncoordinates\n-0.151\n0.190\n0.860\n0.680\n1.051\n\n\ndatathonlocality\n0.035\n0.830\n1.036\n0.698\n1.527\n\n\ndatathoncountry\n-0.187\n0.450\n0.829\n0.500\n1.281\n\n\ndatathonhabitat\n-0.055\n0.834\n0.946\n0.611\n1.454\n\n\ndatathonenvironmentalMedium\n-0.533\n0.096\n0.587\n0.276\n1.081\n\n\ndatathonpublication\n-6.054\n0.382\n0.002\n2.713\n466.380\n\n\ndatathonspatiotemp\n-0.211\n0.008\n0.810\n0.680\n0.949"
  },
  {
    "objectID": "Datathon_Data_Wrangling.html#analysis-4",
    "href": "Datathon_Data_Wrangling.html#analysis-4",
    "title": "Metadata Wrangling Code for GEOME Metadatathon",
    "section": "Analysis 4",
    "text": "Analysis 4\ngained enough metadata to have year & (locality OR coords) for >= 50% of samples; only gains that flip metadata from F/SOME to MOST/T count P(recovered all data|author response = T) ~ registration date of BioProject\n\nm4Betas <- read_delim(\"figures_and_outputs/analysis4_betaTable.csv\",\n                      delim = \" \") %>% \n          #split the CI column\n          mutate(CI95 = str_extract_all(`95% CI`,\"-*[0-9]\\\\.[0-9]+\")) %>%\n          # move the resulting list column into two columns\n          unnest_wider(CI95, names_sep = \"_\") %>% \n          #change to numeric\n          mutate(across(starts_with(\"CI\"),as.numeric)) %>% \n          #exponentiate to get odds ratio\n          mutate(OddsMean = exp(mean), OddsLo95CI = exp(CI95_1), \n                 OddsHi95CI = exp(CI95_2))\n\nRows: 12 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): model, 95% CI\ndbl (3): mean, median, p-value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nkable(m4Betas[,c(1,2,5,8:10)], digits=3)\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\nmean\np-value\nOddsMean\nOddsLo95CI\nOddsHi95CI\n\n\n\n\ndatathonderivedGeneticDataX\n-0.358\n0.080\n0.699\n0.435\n1.051\n\n\ndatathonpermitInformation\n-0.564\n0.058\n0.569\n0.281\n1.015\n\n\ndatathonyearCollected\n-0.204\n0.026\n0.815\n0.680\n0.969\n\n\ndatathonmaterialSampleID\n-0.221\n0.292\n0.802\n0.527\n1.229\n\n\ndatathonspatiotemp\n-0.200\n0.046\n0.819\n0.671\n0.994\n\n\ndatathonpreservative\n-0.389\n0.038\n0.678\n0.460\n0.978\n\n\ndatathoncoordinates\n-0.234\n0.094\n0.791\n0.598\n1.034\n\n\ndatathonlocality\n0.251\n0.312\n1.285\n0.808\n2.201\n\n\ndatathonhabitat\n-0.071\n0.750\n0.931\n0.598\n1.406\n\n\ndatathoncountry\n0.140\n0.622\n1.150\n0.636\n2.012\n\n\ndatathonenvironmentalMedium\n-0.571\n0.076\n0.565\n0.264\n1.034\n\n\ndatathonpublication\n-6.009\n0.458\n0.002\n5.053\n237.460"
  }
]